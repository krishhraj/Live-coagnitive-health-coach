# Live Cognitive Health Coach 

## 1. Project Overview

The **Live Cognitive Health Coach** is a **multimodal fatigue estimation system with an NLP core**.

It combines:
- **NLP**: speech-to-text (Whisper), sentiment analysis, prosody (RMS, pitch, words-per-second), and natural-language recommendations.
- **Vision**: facial emotion recognition (ResNet18) and eye-blink dynamics (EAR, blink rate) using MediaPipe.
- **Rule-based fusion**: merges linguistic, paralinguistic, and visual signals into a single fatigue score and generates live guidance.

The **primary intelligence and interpretation live in the NLP stack**. Vision is a supporting modality.

---

## 2. Key Components

### 2.1 NLP Components

1. **Automatic Speech Recognition (ASR)**
   - Uses **Whisper** to convert microphone or audio-file speech to text.
   - This transcript is the basis for all downstream linguistic reasoning.

2. **Sentiment / Polarity Analysis**
   - Lightweight lexical sentiment model on the transcript.
   - Outputs a score in `[-1, 1]` (negative → positive).
   - This value directly affects the fatigue score (more negative → more fatigued / stressed).

3. **Speech Prosody (Paralinguistic NLP)**
   - Extracted using `librosa`:
     - **RMS** (energy)
     - **Pitch** (f0 via YIN)
     - **WPS** (words per second)
   - Used as proxies for:
     - mental fatigue,
     - cognitive load,
     - emotional dulling or agitation.

4. **Natural Language Generation (NLG)**
   - Rule-based generation of short recommendations:
     - guided breathing,
     - stretching,
     - microbreaks,
     - “brain dump” instructions.
   - Text is determined by the fused fatigue score and sentiment.

### 2.2 Vision / Physiological Components

1. **Facial Emotion Recognition**
   - ResNet18 classifier with 4 classes:
     - `angry`, `happy`, `neutral`, `sad`.
   - Model weights loaded from `models/face_emotion_resnet18.pt`.

2. **Eye Aspect Ratio (EAR) & Blink Rate**
   - MediaPipe FaceMesh landmarks used to compute EAR.
   - Blink events detected from EAR drops.
   - Blink rate (blinks/min) used as a fatigue / drowsiness indicator.

### 2.3 Fusion

All signals are fused into a single fatigue score in `[0, 1]`:
- Emotion label + confidence
- Sentiment score
- RMS, pitch, WPS
- EAR, blink rate

The recommendation engine then maps this fatigue score (and sentiment) into an English instruction string.

---

## 3. Repository Structure

```text
live_coach/
│
├─ live_coach.py          # Main live pipeline: webcam + mic + Whisper + FaceMesh + fusion
├─ infer.py               # Inference on webcam / video + optional audio file
├─ train.py               # Calibration script for EAR/blink thresholds (no model training)
├─ README.md              # (Original readme)
├─ models/
│   └─ face_emotion_resnet18.pt   # ResNet18 emotion classifier (not trained here)
├─ data/
│   └─ processed/
│       ├─ live_logs.csv          # Logs from live_coach.py (created at runtime)
│       └─ infer_logs.csv         # Logs from infer.py (created at runtime)
├─ config.json            # Calibration config (EAR thresholds, blink stats)
└─ requirements.txt       # Dependencies
```

Notes:
- The **face emotion model** is assumed to exist at `models/face_emotion_resnet18.pt`.
- The **EAR and blink thresholds** come from `config.json` generated by `train.py`.

---

## 4. Requirements

- Python 3.8+
- GPU **recommended** (for Whisper + ResNet18, but CPU will still work, just slower)

Core libraries:
- `torch`, `torchvision`
- `opencv-python`
- `mediapipe`
- `sounddevice`
- `scipy`
- `librosa`
- `whisper` (openai-whisper)
- `numpy`
- `pillow`

### Install Dependencies

From the project root:

```bash
pip install -r requirements.txt
```

Make sure **ffmpeg** is installed and on your PATH (needed by Whisper).

---

## 5. Calibration (EAR / Blink Baseline)

Before running the live coach, calibrate your EAR/blink thresholds per user.

```bash
python train.py --duration 15 --out config.json
```

- `--duration`: seconds to record baseline (default: 15)
- `--out`: path to output config file (default: `config.json`)

What this does:
- Opens your webcam.
- Tracks face landmarks via MediaPipe FaceMesh.
- Computes EAR per frame.
- Estimates:
  - mean/median EAR,
  - standard deviation,
  - blink count and blinks per minute.
- Suggests:
  - `EAR_threshold`,
  - `closure_time`,
  - `smoothing_window`.
- Writes everything into `config.json`.

This file is later loaded by `live_coach.py` and (optionally) `infer.py`.

---

## 6. Running the Live Coach (Core NLP Experience)

The **main NLP-centered experience** is `live_coach.py`, which uses:
- mic → audio → Whisper → text,
- text → sentiment,
- audio → prosody,
- camera → face emotion + EAR/blinks,
- fusion → fatigue,
- fatigue → NLG recommendations.

### Basic Run

```bash
python live_coach.py --cam 0 --whisper_model medium
```

Key arguments:
- `--cam`: webcam index (default `0`).
- `--whisper_model`: Whisper size (`tiny`, `base`, `small`, `medium`, `large`).
- `--video_interval`: seconds between emotion snapshots.
- `--audio_duration`: seconds of audio recorded per cycle.
- `--ear_window`: seconds of EAR smoothing for blink-rate computation.
- `--logcsv`: where to save CSV logs (default `data/processed/live_logs.csv`).
- `--config`: path to calibration config (default `config.json`).

Example:

```bash
python live_coach.py     --cam 0     --video_interval 5     --audio_duration 6     --whisper_model medium     --logcsv data/processed/live_logs.csv     --config config.json
```

During each cycle:
1. Video stage:
   - Detects/crops face, runs emotion classifier.
   - Tracks EAR and blink rate.
2. Audio stage:
   - Records a few seconds of audio.
   - Runs Whisper → transcript.
   - Extracts RMS, pitch, WPS.
   - Runs sentiment.
   - Fuses all features into fatigue.
   - Generates a natural-language recommendation.

You see:
- A fatigue bar.
- Emotion label + confidence.
- Audio metrics (RMS, pitch, WPS).
- Recommendation text.
- Live status (video/audio state).
- FPS.

All samples are logged to CSV.

---

## 7. Offline / Scripted Inference (`infer.py`)

`infer.py` lets you run similar fusion on:
- a live webcam feed, or
- a video file,
- plus an optional pre-recorded audio file.

### Example 1: Webcam + One-Time Audio

```bash
python infer.py --video 0 --audio input.wav --whisper_model medium
```

### Example 2: Video File + Audio File

```bash
python infer.py --video path/to/video.mp4 --audio path/to/audio.wav
```

Arguments:
- `--video`: webcam index (`0`, `1`, …) **or** path to a video file.
- `--audio`: path to audio (wav/mp3) for single transcription & prosody.
- `--text`: optional additional text to inject into sentiment.
- `--skip`: process every Nth frame (perf knob).
- `--resize`: resize longest frame edge before processing.
- `--save_csv`: CSV log path (default `data/processed/infer_logs.csv`).
- `--whisper_model`: Whisper variant to use.
- `--config`: optional calibration config.

This mode:
- Loads the face emotion model.
- Optionally analyzes audio once (ASR + prosody + sentiment).
- Processes video frames periodically:
  - detects & crops face,
  - runs emotion model,
  - computes fatigue (with audio-based features fixed),
  - renders overlay and logs metrics.

---

## 8. Logged Data (for NLP / Analysis)

Both `live_coach.py` and `infer.py` log detailed rows like:

```csv
ts,
emo,
emo_conf,
ear,
blink_rate,
text,
sent_score,
rms,
pitch,
wps,
fatigue,
rec
```

This log can be used for:
- NLP experiments on spoken language vs fatigue.
- Prosody–sentiment–fatigue correlation studies.
- Time-series analysis of mental state.
- Evaluating different sentiment models vs the simple lexicon currently used.

---


## 9. Limitations and Extensions

Current limitations:
- Sentiment analysis is lexicon-based (very simple).
- Emotion model is limited to 4 classes and is assumed pre-trained.
- Fusion is rule-based, not learned.

Possible extensions:
- Replace lexicon sentiment with a transformer-based sentiment model.
- Train a small multimodal classifier over the logged CSV.
- Add topic detection on transcripts (e.g., work vs personal stress).
- Swap rule-based NLG with a small template+slot system or a distilled LLM for richer advice.

---

## 10. Summary

This codebase is **not just a CV toy**, it is a **full NLP-centric multimodal system**:

- **ASR** (Whisper) to ground speech in text.  
- **Sentiment + prosody** to interpret emotional and cognitive state.  
- **Fusion** with visual cues to refine fatigue estimation.  
- **NLG** to produce actionable, natural-language recommendations in real time.

Use it as:
- a project demo for **NLP+Speech+Multimodal**,
- a base for research on **cognitive load estimation**,
- or a pipeline to test better NLP components (sentiment, prosody, NLG) in the loop.
